{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnettf.ipynb",
      "provenance": [],
      "mount_file_id": "1mcLrPrC9aatpyKNyD90IYSd4gfat4qL2",
      "authorship_tag": "ABX9TyP5zjQLe+E6Qz7IEuZdGMcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quillbolt/colabnotebook/blob/main/resnettf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5rxByopC-MN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae10d9e4-3432-4cea-d90b-acec85c03cc1"
      },
      "source": [
        "import cv2\n",
        "cv2.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.1.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u76Uoz9A_ahR"
      },
      "source": [
        "import torch\n",
        "mobimodel = torch.load('/content/drive/My Drive/retinaface-WANDB/Resnet50_Final.pth',map_location='cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chmSWo3E_aji"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTKXHhWl_a0K"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models._utils as _utils\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def conv_bn(inp, oup, stride = 1, leaky = 0):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
        "    )\n",
        "\n",
        "def conv_bn_no_relu(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "    )\n",
        "\n",
        "def conv_bn1X1(inp, oup, stride, leaky=0):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
        "    )\n",
        "\n",
        "def conv_dw(inp, oup, stride, leaky=0.1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
        "        nn.BatchNorm2d(inp),\n",
        "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
        "\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
        "    )\n",
        "\n",
        "class SSH(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(SSH, self).__init__()\n",
        "        assert out_channel % 4 == 0\n",
        "        leaky = 0\n",
        "        if (out_channel <= 64):\n",
        "            leaky = 0.1\n",
        "        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1)\n",
        "\n",
        "        self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky)\n",
        "        self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
        "\n",
        "        self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky)\n",
        "        self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        conv3X3 = self.conv3X3(input)\n",
        "\n",
        "        conv5X5_1 = self.conv5X5_1(input)\n",
        "        conv5X5 = self.conv5X5_2(conv5X5_1)\n",
        "\n",
        "        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n",
        "        conv7X7 = self.conv7x7_3(conv7X7_2)\n",
        "\n",
        "        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self,in_channels_list,out_channels):\n",
        "        super(FPN,self).__init__()\n",
        "        leaky = 0\n",
        "        if (out_channels <= 64):\n",
        "            leaky = 0.1\n",
        "        self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride = 1, leaky = leaky)\n",
        "        self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride = 1, leaky = leaky)\n",
        "        self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride = 1, leaky = leaky)\n",
        "\n",
        "        self.merge1 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
        "        self.merge2 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # names = list(input.keys())\n",
        "        input = list(input.values())\n",
        "\n",
        "        output1 = self.output1(input[0])\n",
        "        output2 = self.output2(input[1])\n",
        "        output3 = self.output3(input[2])\n",
        "\n",
        "        up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\"nearest\")\n",
        "        output2 = output2 + up3\n",
        "        output2 = self.merge2(output2)\n",
        "\n",
        "        up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\"nearest\")\n",
        "        output1 = output1 + up2\n",
        "        output1 = self.merge1(output1)\n",
        "\n",
        "        out = [output1, output2, output3]\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MobileNetV1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MobileNetV1, self).__init__()\n",
        "        self.stage1 = nn.Sequential(\n",
        "            conv_bn(3, 8, 2, leaky = 0.1),    # 3\n",
        "            conv_dw(8, 16, 1),   # 7\n",
        "            conv_dw(16, 32, 2),  # 11\n",
        "            conv_dw(32, 32, 1),  # 19\n",
        "            conv_dw(32, 64, 2),  # 27\n",
        "            conv_dw(64, 64, 1),  # 43\n",
        "        )\n",
        "        self.stage2 = nn.Sequential(\n",
        "            conv_dw(64, 128, 2),  # 43 + 16 = 59\n",
        "            conv_dw(128, 128, 1), # 59 + 32 = 91\n",
        "            conv_dw(128, 128, 1), # 91 + 32 = 123\n",
        "            conv_dw(128, 128, 1), # 123 + 32 = 155\n",
        "            conv_dw(128, 128, 1), # 155 + 32 = 187\n",
        "            conv_dw(128, 128, 1), # 187 + 32 = 219\n",
        "        )\n",
        "        self.stage3 = nn.Sequential(\n",
        "            conv_dw(128, 256, 2), # 219 +3 2 = 241\n",
        "            conv_dw(256, 256, 1), # 241 + 64 = 301\n",
        "        )\n",
        "        self.avg = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(256, 1000)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.avg(x)\n",
        "        # x = self.model(x)\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2Jer8ndCC5F"
      },
      "source": [
        "mobimodel = MobileNetV1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arkr5FO7EmOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efbfe80-ac01-43af-d12b-a1f5cfe5536c"
      },
      "source": [
        "summary(mobimodel,(3,224,224))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 112, 112]             216\n",
            "       BatchNorm2d-2          [-1, 8, 112, 112]              16\n",
            "         LeakyReLU-3          [-1, 8, 112, 112]               0\n",
            "            Conv2d-4          [-1, 8, 112, 112]              72\n",
            "       BatchNorm2d-5          [-1, 8, 112, 112]              16\n",
            "         LeakyReLU-6          [-1, 8, 112, 112]               0\n",
            "            Conv2d-7         [-1, 16, 112, 112]             128\n",
            "       BatchNorm2d-8         [-1, 16, 112, 112]              32\n",
            "         LeakyReLU-9         [-1, 16, 112, 112]               0\n",
            "           Conv2d-10           [-1, 16, 56, 56]             144\n",
            "      BatchNorm2d-11           [-1, 16, 56, 56]              32\n",
            "        LeakyReLU-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 32, 56, 56]             512\n",
            "      BatchNorm2d-14           [-1, 32, 56, 56]              64\n",
            "        LeakyReLU-15           [-1, 32, 56, 56]               0\n",
            "           Conv2d-16           [-1, 32, 56, 56]             288\n",
            "      BatchNorm2d-17           [-1, 32, 56, 56]              64\n",
            "        LeakyReLU-18           [-1, 32, 56, 56]               0\n",
            "           Conv2d-19           [-1, 32, 56, 56]           1,024\n",
            "      BatchNorm2d-20           [-1, 32, 56, 56]              64\n",
            "        LeakyReLU-21           [-1, 32, 56, 56]               0\n",
            "           Conv2d-22           [-1, 32, 28, 28]             288\n",
            "      BatchNorm2d-23           [-1, 32, 28, 28]              64\n",
            "        LeakyReLU-24           [-1, 32, 28, 28]               0\n",
            "           Conv2d-25           [-1, 64, 28, 28]           2,048\n",
            "      BatchNorm2d-26           [-1, 64, 28, 28]             128\n",
            "        LeakyReLU-27           [-1, 64, 28, 28]               0\n",
            "           Conv2d-28           [-1, 64, 28, 28]             576\n",
            "      BatchNorm2d-29           [-1, 64, 28, 28]             128\n",
            "        LeakyReLU-30           [-1, 64, 28, 28]               0\n",
            "           Conv2d-31           [-1, 64, 28, 28]           4,096\n",
            "      BatchNorm2d-32           [-1, 64, 28, 28]             128\n",
            "        LeakyReLU-33           [-1, 64, 28, 28]               0\n",
            "           Conv2d-34           [-1, 64, 14, 14]             576\n",
            "      BatchNorm2d-35           [-1, 64, 14, 14]             128\n",
            "        LeakyReLU-36           [-1, 64, 14, 14]               0\n",
            "           Conv2d-37          [-1, 128, 14, 14]           8,192\n",
            "      BatchNorm2d-38          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-39          [-1, 128, 14, 14]               0\n",
            "           Conv2d-40          [-1, 128, 14, 14]           1,152\n",
            "      BatchNorm2d-41          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-42          [-1, 128, 14, 14]               0\n",
            "           Conv2d-43          [-1, 128, 14, 14]          16,384\n",
            "      BatchNorm2d-44          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-45          [-1, 128, 14, 14]               0\n",
            "           Conv2d-46          [-1, 128, 14, 14]           1,152\n",
            "      BatchNorm2d-47          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-48          [-1, 128, 14, 14]               0\n",
            "           Conv2d-49          [-1, 128, 14, 14]          16,384\n",
            "      BatchNorm2d-50          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-51          [-1, 128, 14, 14]               0\n",
            "           Conv2d-52          [-1, 128, 14, 14]           1,152\n",
            "      BatchNorm2d-53          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-54          [-1, 128, 14, 14]               0\n",
            "           Conv2d-55          [-1, 128, 14, 14]          16,384\n",
            "      BatchNorm2d-56          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-57          [-1, 128, 14, 14]               0\n",
            "           Conv2d-58          [-1, 128, 14, 14]           1,152\n",
            "      BatchNorm2d-59          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-60          [-1, 128, 14, 14]               0\n",
            "           Conv2d-61          [-1, 128, 14, 14]          16,384\n",
            "      BatchNorm2d-62          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-63          [-1, 128, 14, 14]               0\n",
            "           Conv2d-64          [-1, 128, 14, 14]           1,152\n",
            "      BatchNorm2d-65          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-66          [-1, 128, 14, 14]               0\n",
            "           Conv2d-67          [-1, 128, 14, 14]          16,384\n",
            "      BatchNorm2d-68          [-1, 128, 14, 14]             256\n",
            "        LeakyReLU-69          [-1, 128, 14, 14]               0\n",
            "           Conv2d-70            [-1, 128, 7, 7]           1,152\n",
            "      BatchNorm2d-71            [-1, 128, 7, 7]             256\n",
            "        LeakyReLU-72            [-1, 128, 7, 7]               0\n",
            "           Conv2d-73            [-1, 256, 7, 7]          32,768\n",
            "      BatchNorm2d-74            [-1, 256, 7, 7]             512\n",
            "        LeakyReLU-75            [-1, 256, 7, 7]               0\n",
            "           Conv2d-76            [-1, 256, 7, 7]           2,304\n",
            "      BatchNorm2d-77            [-1, 256, 7, 7]             512\n",
            "        LeakyReLU-78            [-1, 256, 7, 7]               0\n",
            "           Conv2d-79            [-1, 256, 7, 7]          65,536\n",
            "      BatchNorm2d-80            [-1, 256, 7, 7]             512\n",
            "        LeakyReLU-81            [-1, 256, 7, 7]               0\n",
            "AdaptiveAvgPool2d-82            [-1, 256, 1, 1]               0\n",
            "           Linear-83                 [-1, 1000]         257,000\n",
            "================================================================\n",
            "Total params: 470,072\n",
            "Trainable params: 470,072\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 28.86\n",
            "Params size (MB): 1.79\n",
            "Estimated Total Size (MB): 31.23\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0aCA1BFGqM9"
      },
      "source": [
        "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from collections import namedtuple\n",
        "import math\n",
        "import pdb\n",
        "\n",
        "##################################  Original Arcface Model #############################################################\n",
        "\n",
        "class Flatten(Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "def l2_norm(input,axis=1):\n",
        "    norm = torch.norm(input,2,axis,True)\n",
        "    output = torch.div(input, norm)\n",
        "    return output\n",
        "\n",
        "class SEModule(Module):\n",
        "    def __init__(self, channels, reduction):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.avg_pool = AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = Conv2d(\n",
        "            channels, channels // reduction, kernel_size=1, padding=0 ,bias=False)\n",
        "        self.relu = ReLU(inplace=True)\n",
        "        self.fc2 = Conv2d(\n",
        "            channels // reduction, channels, kernel_size=1, padding=0 ,bias=False)\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        module_input = x\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return module_input * x\n",
        "\n",
        "class bottleneck_IR(Module):\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(bottleneck_IR, self).__init__()\n",
        "        if in_channel == depth:\n",
        "            self.shortcut_layer = MaxPool2d(1, stride)\n",
        "        else:\n",
        "            self.shortcut_layer = Sequential(\n",
        "                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), BatchNorm2d(depth))\n",
        "        self.res_layer = Sequential(\n",
        "            BatchNorm2d(in_channel),\n",
        "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1 ,bias=False), PReLU(depth),\n",
        "            Conv2d(depth, depth, (3, 3), stride, 1 ,bias=False), BatchNorm2d(depth))\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut_layer(x)\n",
        "        res = self.res_layer(x)\n",
        "        return res + shortcut\n",
        "\n",
        "class bottleneck_IR_SE(Module):\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(bottleneck_IR_SE, self).__init__()\n",
        "        if in_channel == depth:\n",
        "            self.shortcut_layer = MaxPool2d(1, stride)\n",
        "        else:\n",
        "            self.shortcut_layer = Sequential(\n",
        "                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), \n",
        "                BatchNorm2d(depth))\n",
        "        self.res_layer = Sequential(\n",
        "            BatchNorm2d(in_channel),\n",
        "            Conv2d(in_channel, depth, (3,3), (1,1),1 ,bias=False),\n",
        "            PReLU(depth),\n",
        "            Conv2d(depth, depth, (3,3), stride, 1 ,bias=False),\n",
        "            BatchNorm2d(depth),\n",
        "            SEModule(depth,16)\n",
        "            )\n",
        "    def forward(self,x):\n",
        "        shortcut = self.shortcut_layer(x)\n",
        "        res = self.res_layer(x)\n",
        "        return res + shortcut\n",
        "\n",
        "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
        "    '''A named tuple describing a ResNet block.'''\n",
        "    \n",
        "def get_block(in_channel, depth, num_units, stride = 2):\n",
        "    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units-1)]\n",
        "\n",
        "def get_blocks(num_layers):\n",
        "    if num_layers == 50:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units = 3),\n",
        "            get_block(in_channel=64, depth=128, num_units=4),\n",
        "            get_block(in_channel=128, depth=256, num_units=14),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 100:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=13),\n",
        "            get_block(in_channel=128, depth=256, num_units=30),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 152:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=8),\n",
        "            get_block(in_channel=128, depth=256, num_units=36),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    return blocks\n",
        "\n",
        "class Backbone(Module):\n",
        "    def __init__(self, num_layers, drop_ratio, mode='ir'):\n",
        "        super(Backbone, self).__init__()\n",
        "        assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n",
        "        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n",
        "        blocks = get_blocks(num_layers)\n",
        "        if mode == 'ir':\n",
        "            unit_module = bottleneck_IR\n",
        "        elif mode == 'ir_se':\n",
        "            unit_module = bottleneck_IR_SE\n",
        "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1 ,bias=False), \n",
        "                                      BatchNorm2d(64), \n",
        "                                      PReLU(64))\n",
        "        self.output_layer = Sequential(BatchNorm2d(512), \n",
        "                                       Dropout(drop_ratio),\n",
        "                                       Flatten(),\n",
        "                                       Linear(512 * 7 * 7, 512),\n",
        "                                       BatchNorm1d(512))\n",
        "        modules = []\n",
        "        for block in blocks:\n",
        "            for bottleneck in block:\n",
        "                modules.append(\n",
        "                    unit_module(bottleneck.in_channel,\n",
        "                                bottleneck.depth,\n",
        "                                bottleneck.stride))\n",
        "        self.body = Sequential(*modules)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.body(x)\n",
        "        x = self.output_layer(x)\n",
        "        return l2_norm(x)\n",
        "\n",
        "##################################  MobileFaceNet #############################################################\n",
        "    \n",
        "class Conv_block(Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Conv_block, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "        self.prelu = PReLU(out_c)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.prelu(x)\n",
        "        return x\n",
        "\n",
        "class Linear_block(Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Linear_block, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "class Depth_Wise(Module):\n",
        "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
        "        super(Depth_Wise, self).__init__()\n",
        "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
        "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.residual = residual\n",
        "     def forward(self, x):\n",
        "        if self.residual:\n",
        "            short_cut = x\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_dw(x)\n",
        "        x = self.project(x)\n",
        "        if self.residual:\n",
        "            output = short_cut + x\n",
        "        else:\n",
        "            output = x\n",
        "        return output\n",
        "\n",
        "class Residual(Module):\n",
        "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
        "        super(Residual, self).__init__()\n",
        "        modules = []\n",
        "        for _ in range(num_block):\n",
        "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
        "        self.model = Sequential(*modules)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class MobileFaceNet(Module):\n",
        "    def __init__(self, embedding_size):\n",
        "        super(MobileFaceNet, self).__init__()\n",
        "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
        "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
        "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
        "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
        "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
        "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
        "        self.conv_6_flatten = Flatten()\n",
        "        self.linear = Linear(512, embedding_size, bias=False)\n",
        "        self.bn = BatchNorm1d(embedding_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        out = self.conv2_dw(out)\n",
        "\n",
        "        out = self.conv_23(out)\n",
        "\n",
        "        out = self.conv_3(out)\n",
        "        \n",
        "        out = self.conv_34(out)\n",
        "\n",
        "        out = self.conv_4(out)\n",
        "\n",
        "        out = self.conv_45(out)\n",
        "\n",
        "        out = self.conv_5(out)\n",
        "\n",
        "        out = self.conv_6_sep(out)\n",
        "\n",
        "        out = self.conv_6_dw(out)\n",
        "\n",
        "        out = self.conv_6_flatten(out)\n",
        "\n",
        "        out = self.linear(out)\n",
        "\n",
        "        out = self.bn(out)\n",
        "        return l2_norm(out)\n",
        "\n",
        "##################################  Arcface head #############################################################\n",
        "\n",
        "class Arcface(Module):\n",
        "    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \n",
        "    def __init__(self, embedding_size=512, classnum=51332,  s=64., m=0.5):\n",
        "        super(Arcface, self).__init__()\n",
        "        self.classnum = classnum\n",
        "        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\n",
        "        # initial kernel\n",
        "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
        "        self.m = m # the margin value, default is 0.5\n",
        "        self.s = s # scalar value default is 64, see normface https://arxiv.org/abs/1704.06369\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.mm = self.sin_m * m  # issue 1\n",
        "        self.threshold = math.cos(math.pi - m)\n",
        "    def forward(self, embbedings, label):\n",
        "        # weights norm\n",
        "        nB = len(embbedings)\n",
        "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
        "        # cos(theta+m)\n",
        "        cos_theta = torch.mm(embbedings,kernel_norm)\n",
        "#         output = torch.mm(embbedings,kernel_norm)\n",
        "        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\n",
        "        cos_theta_2 = torch.pow(cos_theta, 2)\n",
        "        sin_theta_2 = 1 - cos_theta_2\n",
        "        sin_theta = torch.sqrt(sin_theta_2)\n",
        "        cos_theta_m = (cos_theta * self.cos_m - sin_theta * self.sin_m)\n",
        "        # this condition controls the theta+m should in range [0, pi]\n",
        "        #      0<=theta+m<=pi\n",
        "        #     -m<=theta<=pi-m\n",
        "        cond_v = cos_theta - self.threshold\n",
        "        cond_mask = cond_v <= 0\n",
        "        keep_val = (cos_theta - self.mm) # when theta not in [0,pi], use cosface instead\n",
        "        cos_theta_m[cond_mask] = keep_val[cond_mask]\n",
        "        output = cos_theta * 1.0 # a little bit hacky way to prevent in_place operation on cos_theta\n",
        "        idx_ = torch.arange(0, nB, dtype=torch.long)\n",
        "        output[idx_, label] = cos_theta_m[idx_, label]\n",
        "        output *= self.s # scale up in order to make softmax work, first introduced in normface\n",
        "        return output\n",
        "\n",
        "##################################  Cosface head #############################################################    \n",
        "    \n",
        "class Am_softmax(Module):\n",
        "    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \n",
        "    def __init__(self,embedding_size=512,classnum=51332):\n",
        "        super(Am_softmax, self).__init__()\n",
        "        self.classnum = classnum\n",
        "        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\n",
        "        # initial kernel\n",
        "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
        "        self.m = 0.35 # additive margin recommended by the paper\n",
        "        self.s = 30. # see normface https://arxiv.org/abs/1704.06369\n",
        "    def forward(self,embbedings,label):\n",
        "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
        "        cos_theta = torch.mm(embbedings,kernel_norm)\n",
        "        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\n",
        "        phi = cos_theta - self.m\n",
        "        label = label.view(-1,1) #size=(B,1)\n",
        "        index = cos_theta.data * 0.0 #size=(B,Classnum)\n",
        "        index.scatter_(1,label.data.view(-1,1),1)\n",
        "        index = index.byte()\n",
        "        output = cos_theta * 1.0\n",
        "        output[index] = phi[index] #only change the correct predicted output\n",
        "        output *= self.s # scale up in order to make softmax work, first introduced in normface\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfWKbj5Pr0bF"
      },
      "source": [
        "from torchsummary import  summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqsIrMc_GOEs"
      },
      "source": [
        "model = MobileFaceNet(512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBu1vmQmGWas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccf07fd-6afb-4cf9-a97c-4a4ba5e5ef2b"
      },
      "source": [
        "summary(model, (3,112,112))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 56, 56]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 56, 56]             128\n",
            "             PReLU-3           [-1, 64, 56, 56]              64\n",
            "        Conv_block-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]             576\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "             PReLU-7           [-1, 64, 56, 56]              64\n",
            "        Conv_block-8           [-1, 64, 56, 56]               0\n",
            "            Conv2d-9          [-1, 128, 56, 56]           8,192\n",
            "      BatchNorm2d-10          [-1, 128, 56, 56]             256\n",
            "            PReLU-11          [-1, 128, 56, 56]             128\n",
            "       Conv_block-12          [-1, 128, 56, 56]               0\n",
            "           Conv2d-13          [-1, 128, 28, 28]           1,152\n",
            "      BatchNorm2d-14          [-1, 128, 28, 28]             256\n",
            "            PReLU-15          [-1, 128, 28, 28]             128\n",
            "       Conv_block-16          [-1, 128, 28, 28]               0\n",
            "           Conv2d-17           [-1, 64, 28, 28]           8,192\n",
            "      BatchNorm2d-18           [-1, 64, 28, 28]             128\n",
            "     Linear_block-19           [-1, 64, 28, 28]               0\n",
            "       Depth_Wise-20           [-1, 64, 28, 28]               0\n",
            "           Conv2d-21          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-22          [-1, 128, 28, 28]             256\n",
            "            PReLU-23          [-1, 128, 28, 28]             128\n",
            "       Conv_block-24          [-1, 128, 28, 28]               0\n",
            "           Conv2d-25          [-1, 128, 28, 28]           1,152\n",
            "      BatchNorm2d-26          [-1, 128, 28, 28]             256\n",
            "            PReLU-27          [-1, 128, 28, 28]             128\n",
            "       Conv_block-28          [-1, 128, 28, 28]               0\n",
            "           Conv2d-29           [-1, 64, 28, 28]           8,192\n",
            "      BatchNorm2d-30           [-1, 64, 28, 28]             128\n",
            "     Linear_block-31           [-1, 64, 28, 28]               0\n",
            "       Depth_Wise-32           [-1, 64, 28, 28]               0\n",
            "           Conv2d-33          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-34          [-1, 128, 28, 28]             256\n",
            "            PReLU-35          [-1, 128, 28, 28]             128\n",
            "       Conv_block-36          [-1, 128, 28, 28]               0\n",
            "           Conv2d-37          [-1, 128, 28, 28]           1,152\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "            PReLU-39          [-1, 128, 28, 28]             128\n",
            "       Conv_block-40          [-1, 128, 28, 28]               0\n",
            "           Conv2d-41           [-1, 64, 28, 28]           8,192\n",
            "      BatchNorm2d-42           [-1, 64, 28, 28]             128\n",
            "     Linear_block-43           [-1, 64, 28, 28]               0\n",
            "       Depth_Wise-44           [-1, 64, 28, 28]               0\n",
            "           Conv2d-45          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "            PReLU-47          [-1, 128, 28, 28]             128\n",
            "       Conv_block-48          [-1, 128, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]           1,152\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "            PReLU-51          [-1, 128, 28, 28]             128\n",
            "       Conv_block-52          [-1, 128, 28, 28]               0\n",
            "           Conv2d-53           [-1, 64, 28, 28]           8,192\n",
            "      BatchNorm2d-54           [-1, 64, 28, 28]             128\n",
            "     Linear_block-55           [-1, 64, 28, 28]               0\n",
            "       Depth_Wise-56           [-1, 64, 28, 28]               0\n",
            "           Conv2d-57          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
            "            PReLU-59          [-1, 128, 28, 28]             128\n",
            "       Conv_block-60          [-1, 128, 28, 28]               0\n",
            "           Conv2d-61          [-1, 128, 28, 28]           1,152\n",
            "      BatchNorm2d-62          [-1, 128, 28, 28]             256\n",
            "            PReLU-63          [-1, 128, 28, 28]             128\n",
            "       Conv_block-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65           [-1, 64, 28, 28]           8,192\n",
            "      BatchNorm2d-66           [-1, 64, 28, 28]             128\n",
            "     Linear_block-67           [-1, 64, 28, 28]               0\n",
            "       Depth_Wise-68           [-1, 64, 28, 28]               0\n",
            "         Residual-69           [-1, 64, 28, 28]               0\n",
            "           Conv2d-70          [-1, 256, 28, 28]          16,384\n",
            "      BatchNorm2d-71          [-1, 256, 28, 28]             512\n",
            "            PReLU-72          [-1, 256, 28, 28]             256\n",
            "       Conv_block-73          [-1, 256, 28, 28]               0\n",
            "           Conv2d-74          [-1, 256, 14, 14]           2,304\n",
            "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
            "            PReLU-76          [-1, 256, 14, 14]             256\n",
            "       Conv_block-77          [-1, 256, 14, 14]               0\n",
            "           Conv2d-78          [-1, 128, 14, 14]          32,768\n",
            "      BatchNorm2d-79          [-1, 128, 14, 14]             256\n",
            "     Linear_block-80          [-1, 128, 14, 14]               0\n",
            "       Depth_Wise-81          [-1, 128, 14, 14]               0\n",
            "           Conv2d-82          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
            "            PReLU-84          [-1, 256, 14, 14]             256\n",
            "       Conv_block-85          [-1, 256, 14, 14]               0\n",
            "           Conv2d-86          [-1, 256, 14, 14]           2,304\n",
            "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
            "            PReLU-88          [-1, 256, 14, 14]             256\n",
            "       Conv_block-89          [-1, 256, 14, 14]               0\n",
            "           Conv2d-90          [-1, 128, 14, 14]          32,768\n",
            "      BatchNorm2d-91          [-1, 128, 14, 14]             256\n",
            "     Linear_block-92          [-1, 128, 14, 14]               0\n",
            "       Depth_Wise-93          [-1, 128, 14, 14]               0\n",
            "           Conv2d-94          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
            "            PReLU-96          [-1, 256, 14, 14]             256\n",
            "       Conv_block-97          [-1, 256, 14, 14]               0\n",
            "           Conv2d-98          [-1, 256, 14, 14]           2,304\n",
            "      BatchNorm2d-99          [-1, 256, 14, 14]             512\n",
            "           PReLU-100          [-1, 256, 14, 14]             256\n",
            "      Conv_block-101          [-1, 256, 14, 14]               0\n",
            "          Conv2d-102          [-1, 128, 14, 14]          32,768\n",
            "     BatchNorm2d-103          [-1, 128, 14, 14]             256\n",
            "    Linear_block-104          [-1, 128, 14, 14]               0\n",
            "      Depth_Wise-105          [-1, 128, 14, 14]               0\n",
            "          Conv2d-106          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-107          [-1, 256, 14, 14]             512\n",
            "           PReLU-108          [-1, 256, 14, 14]             256\n",
            "      Conv_block-109          [-1, 256, 14, 14]               0\n",
            "          Conv2d-110          [-1, 256, 14, 14]           2,304\n",
            "     BatchNorm2d-111          [-1, 256, 14, 14]             512\n",
            "           PReLU-112          [-1, 256, 14, 14]             256\n",
            "      Conv_block-113          [-1, 256, 14, 14]               0\n",
            "          Conv2d-114          [-1, 128, 14, 14]          32,768\n",
            "     BatchNorm2d-115          [-1, 128, 14, 14]             256\n",
            "    Linear_block-116          [-1, 128, 14, 14]               0\n",
            "      Depth_Wise-117          [-1, 128, 14, 14]               0\n",
            "          Conv2d-118          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-119          [-1, 256, 14, 14]             512\n",
            "           PReLU-120          [-1, 256, 14, 14]             256\n",
            "      Conv_block-121          [-1, 256, 14, 14]               0\n",
            "          Conv2d-122          [-1, 256, 14, 14]           2,304\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "           PReLU-124          [-1, 256, 14, 14]             256\n",
            "      Conv_block-125          [-1, 256, 14, 14]               0\n",
            "          Conv2d-126          [-1, 128, 14, 14]          32,768\n",
            "     BatchNorm2d-127          [-1, 128, 14, 14]             256\n",
            "    Linear_block-128          [-1, 128, 14, 14]               0\n",
            "      Depth_Wise-129          [-1, 128, 14, 14]               0\n",
            "          Conv2d-130          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-131          [-1, 256, 14, 14]             512\n",
            "           PReLU-132          [-1, 256, 14, 14]             256\n",
            "      Conv_block-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]           2,304\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "           PReLU-136          [-1, 256, 14, 14]             256\n",
            "      Conv_block-137          [-1, 256, 14, 14]               0\n",
            "          Conv2d-138          [-1, 128, 14, 14]          32,768\n",
            "     BatchNorm2d-139          [-1, 128, 14, 14]             256\n",
            "    Linear_block-140          [-1, 128, 14, 14]               0\n",
            "      Depth_Wise-141          [-1, 128, 14, 14]               0\n",
            "          Conv2d-142          [-1, 256, 14, 14]          32,768\n",
            "     BatchNorm2d-143          [-1, 256, 14, 14]             512\n",
            "           PReLU-144          [-1, 256, 14, 14]             256\n",
            "      Conv_block-145          [-1, 256, 14, 14]               0\n",
            "          Conv2d-146          [-1, 256, 14, 14]           2,304\n",
            "     BatchNorm2d-147          [-1, 256, 14, 14]             512\n",
            "           PReLU-148          [-1, 256, 14, 14]             256\n",
            "      Conv_block-149          [-1, 256, 14, 14]               0\n",
            "          Conv2d-150          [-1, 128, 14, 14]          32,768\n",
            "     BatchNorm2d-151          [-1, 128, 14, 14]             256\n",
            "    Linear_block-152          [-1, 128, 14, 14]               0\n",
            "      Depth_Wise-153          [-1, 128, 14, 14]               0\n",
            "        Residual-154          [-1, 128, 14, 14]               0\n",
            "          Conv2d-155          [-1, 512, 14, 14]          65,536\n",
            "     BatchNorm2d-156          [-1, 512, 14, 14]           1,024\n",
            "           PReLU-157          [-1, 512, 14, 14]             512\n",
            "      Conv_block-158          [-1, 512, 14, 14]               0\n",
            "          Conv2d-159            [-1, 512, 7, 7]           4,608\n",
            "     BatchNorm2d-160            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-161            [-1, 512, 7, 7]             512\n",
            "      Conv_block-162            [-1, 512, 7, 7]               0\n",
            "          Conv2d-163            [-1, 128, 7, 7]          65,536\n",
            "     BatchNorm2d-164            [-1, 128, 7, 7]             256\n",
            "    Linear_block-165            [-1, 128, 7, 7]               0\n",
            "      Depth_Wise-166            [-1, 128, 7, 7]               0\n",
            "          Conv2d-167            [-1, 256, 7, 7]          32,768\n",
            "     BatchNorm2d-168            [-1, 256, 7, 7]             512\n",
            "           PReLU-169            [-1, 256, 7, 7]             256\n",
            "      Conv_block-170            [-1, 256, 7, 7]               0\n",
            "          Conv2d-171            [-1, 256, 7, 7]           2,304\n",
            "     BatchNorm2d-172            [-1, 256, 7, 7]             512\n",
            "           PReLU-173            [-1, 256, 7, 7]             256\n",
            "      Conv_block-174            [-1, 256, 7, 7]               0\n",
            "          Conv2d-175            [-1, 128, 7, 7]          32,768\n",
            "     BatchNorm2d-176            [-1, 128, 7, 7]             256\n",
            "    Linear_block-177            [-1, 128, 7, 7]               0\n",
            "      Depth_Wise-178            [-1, 128, 7, 7]               0\n",
            "          Conv2d-179            [-1, 256, 7, 7]          32,768\n",
            "     BatchNorm2d-180            [-1, 256, 7, 7]             512\n",
            "           PReLU-181            [-1, 256, 7, 7]             256\n",
            "      Conv_block-182            [-1, 256, 7, 7]               0\n",
            "          Conv2d-183            [-1, 256, 7, 7]           2,304\n",
            "     BatchNorm2d-184            [-1, 256, 7, 7]             512\n",
            "           PReLU-185            [-1, 256, 7, 7]             256\n",
            "      Conv_block-186            [-1, 256, 7, 7]               0\n",
            "          Conv2d-187            [-1, 128, 7, 7]          32,768\n",
            "     BatchNorm2d-188            [-1, 128, 7, 7]             256\n",
            "    Linear_block-189            [-1, 128, 7, 7]               0\n",
            "      Depth_Wise-190            [-1, 128, 7, 7]               0\n",
            "        Residual-191            [-1, 128, 7, 7]               0\n",
            "          Conv2d-192            [-1, 512, 7, 7]          65,536\n",
            "     BatchNorm2d-193            [-1, 512, 7, 7]           1,024\n",
            "           PReLU-194            [-1, 512, 7, 7]             512\n",
            "      Conv_block-195            [-1, 512, 7, 7]               0\n",
            "          Conv2d-196            [-1, 512, 1, 1]          25,088\n",
            "     BatchNorm2d-197            [-1, 512, 1, 1]           1,024\n",
            "    Linear_block-198            [-1, 512, 1, 1]               0\n",
            "         Flatten-199                  [-1, 512]               0\n",
            "          Linear-200                  [-1, 512]         262,144\n",
            "     BatchNorm1d-201                  [-1, 512]           1,024\n",
            "================================================================\n",
            "Total params: 1,200,512\n",
            "Trainable params: 1,200,512\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.14\n",
            "Forward/backward pass size (MB): 98.45\n",
            "Params size (MB): 4.58\n",
            "Estimated Total Size (MB): 103.18\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJPu71kHUVE"
      },
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/My Drive/face.evoLVe.PyTorch/train'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E1lKlIBX--s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd09805b-5c90-4e6f-b7e4-014241a81221"
      },
      "source": [
        "print(classnames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done', '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkGO2aHsZpHa"
      },
      "source": [
        "def get_image_paths(facedir):\n",
        "    image_paths = []\n",
        "    if os.path.isdir(facedir):\n",
        "        images = os.listdir(facedir)\n",
        "        image_paths = [os.path.join(facedir, img) for img in images]\n",
        "    return image_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnbB_kgzZvAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4508c01-fddd-436b-9723-1cbbe055c4ea"
      },
      "source": [
        "get_image_paths('/content/drive/My Drive/face.evoLVe.PyTorch/train')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen Duy Khanh-MT-013',\n",
              " '/content/drive/My Drive/face.evoLVe.PyTorch/train/Nguyen uc iep-MT-004',\n",
              " '/content/drive/My Drive/face.evoLVe.PyTorch/train/Huynh Tuan Minh - MT-014',\n",
              " '/content/drive/My Drive/face.evoLVe.PyTorch/train/Tran Tien Hoa-MT-002',\n",
              " '/content/drive/My Drive/face.evoLVe.PyTorch/train/MT - 009 o Van Truong - done']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDw39dCDIn5b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGrJ3GwZJaFl"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DSiAARWKM8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d3eebf-0b09-4d89-86d9-466c9921e746"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC7xu1-SKYgZ"
      },
      "source": [
        "transform1 = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWG1-HbkfGod"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from parameters import *\n",
        "import pickle\n",
        "\n",
        "\n",
        "input_shape = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "with open(\"./path_dict.p\", 'rb') as f:\n",
        "    paths = pickle.load(f)\n",
        "    \n",
        "faces = []\n",
        "for key in paths.keys():\n",
        "    paths[key] = paths[key].replace(\"\\\\\", \"/\")\n",
        "    faces.append(key)\n",
        "    \n",
        "images = {}\n",
        "for key in paths.keys():\n",
        "    li = []\n",
        "    for img in os.listdir(paths[key]):\n",
        "        img1 = cv2.imread(os.path.join(paths[key],img))\n",
        "        img2 = img1[...,::-1]\n",
        "        li.append(np.around(np.transpose(img2, (2,0,1))/255.0, decimals=12))\n",
        "    images[key] = np.array(li)\n",
        "\n",
        "\n",
        "def batch_generator(batch_size=16):\n",
        "    y_val = np.zeros((batch_size, 2, 1))\n",
        "    anchors = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
        "    positives = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
        "    negatives = np.zeros((batch_size, input_shape[0], input_shape[1], input_shape[2]))\n",
        "    \n",
        "    while True:\n",
        "        for i in range(batch_size):\n",
        "            positiveFace = faces[np.random.randint(len(faces))]\n",
        "            negativeFace = faces[np.random.randint(len(faces))]\n",
        "            while positiveFace == negativeFace:\n",
        "                negativeFace = faces[np.random.randint(len(faces))]\n",
        "\n",
        "            positives[i] = images[positiveFace][np.random.randint(len(images[positiveFace]))]\n",
        "            anchors[i] = images[positiveFace][np.random.randint(len(images[positiveFace]))]\n",
        "            negatives[i] = images[negativeFace][np.random.randint(len(images[negativeFace]))]\n",
        "        \n",
        "        x_data = {'anchor': anchors,\n",
        "                  'anchorPositive': positives,\n",
        "                  'anchorNegative': negatives\n",
        "                  }\n",
        "        \n",
        "        yield (x_data, [y_val, y_val, y_val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyT-LwqNdCSt"
      },
      "source": [
        "import PIL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqvW5E_CY9uz"
      },
      "source": [
        "transform2 = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(20, resample=PIL.Image.BILINEAR)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl0E2UirVtaK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "4e23fa41-a51b-45d1-f143-8c4139112814"
      },
      "source": [
        "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
        "scheduler = MultiStepLR(optimizer, [5, 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c9780648c730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LhdMwtcLZ9e"
      },
      "source": [
        "def triplet_loss_1(y_true,y_pred):\n",
        "    #print(y_pred)\n",
        "    anchor=y_pred[:,0:128]\n",
        "    pos=y_pred[:,128:256]\n",
        "    neg=y_pred[:,256:384]\n",
        "    \n",
        "    positive_distance = K.sum(K.abs(anchor-pos), axis=1)\n",
        "    negative_distance = K.sum(K.abs(anchor-neg), axis=1)\n",
        "    probs=K.softmax([positive_distance,negative_distance],axis=0)\n",
        "    #loss = positive_distance - negative_distance+alpha\n",
        "    loss=K.mean(K.abs(probs[0])+K.abs(1.0-probs[1]))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekVTknTPU3Kp"
      },
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        \n",
        "    def calc_euclidean(self, x1, x2):\n",
        "        return (x1 - x2).pow(2).sum(1)\n",
        "    \n",
        "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "        distance_positive = self.calc_euclidean(anchor, positive)\n",
        "        distance_negative = self.calc_euclidean(anchor, negative)\n",
        "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "\n",
        "        return losses.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY-rDyU4VMxJ"
      },
      "source": [
        "dataset = datasets.ImageFolder('/content/drive/My Drive/face.evoLVe.PyTorch/train',transform2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThEMONN4cad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff694c00-6e7c-4c73-ae1d-76f56b105ade"
      },
      "source": [
        "np.arange(len(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W154cWANa1wt"
      },
      "source": [
        "import pdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkE_0vizLpm2"
      },
      "source": [
        "def collate_pil(x): \n",
        "    out_x, out_y = [], [] \n",
        "    for xx, yy in x: \n",
        "        out_x.append(xx) \n",
        "        out_y.append(yy) \n",
        "    return out_x, out_y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJZFI5KBLweO"
      },
      "source": [
        "def data_gen(batch_size=32):\n",
        "    while True:\n",
        "        i=0\n",
        "        positive=[]\n",
        "        anchor=[]\n",
        "        negative=[]    \n",
        "        \n",
        "\n",
        "        while(i<batch_size):\n",
        "            r=random.choice(os.listdir(PATH))\n",
        "            p=PATH+'/'+ r\n",
        "            id=os.listdir(p)\n",
        "            ra=random.sample(id,2)\n",
        "            pos_dir=p+'/'+ra[0]\n",
        "            neg_dir=p+'/'+ra[1]\n",
        "            pos=pos_dir+'/'+random.choice(os.listdir(pos_dir))\n",
        "            anc=pos_dir+'/'+random.choice([x for x in os.listdir(pos_dir) ])\n",
        "            neg=neg_dir+'/'+random.choice(os.listdir(neg_dir))\n",
        "            pos_img=localize_resize(pos,path_haar)\n",
        "                    #print(pos+anc+neg)\n",
        "            if pos_img is -1:\n",
        "                continue\n",
        "            neg_img=localize_resize(neg,path_haar)\n",
        "            if neg_img is -1:\n",
        "                continue\n",
        "            anc_img=localize_resize(anc,path_haar)\n",
        "            if anc_img is -1:\n",
        "                continue\n",
        "            positive.append(list(pos_img))\n",
        "                #print('positive{0}'.format(i))\n",
        "            negative.append(list(neg_img))\n",
        "                #print('negative{0}'.format(i))\n",
        "            anchor.append(list(anc_img))\n",
        "                #print('anchor{0}'.format(i))\n",
        "            i=i+1\n",
        "        #return anchor,positive,negative\n",
        "        yield ([np.array(anchor),np.array(positive),np.array(negative)],np.zeros((batch_size,1)).astype(\"float32\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MX-GxPnL0K1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}